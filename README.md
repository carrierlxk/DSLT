# Deep Regression Tracking with Shrinkage Loss

### Introduction

This is the research code for the ECCV 2018 paper: 

[Xiankai Lu](https://github.com/carrierlxk),  [Chao Ma](https://sites.google.com/site/chaoma99/), [Bingbing Ni](https://scholar.google.com/citations?user=eUbmKwYAAAAJ&hl=en), [Xiaokang Yang](http://english.seiee.sjtu.edu.cn/english/detail/842_802.htm), [Ian Reid](https://cs.adelaide.edu.au/~ianr/), and [Ming-Hsuan Yang](http://faculty.ucmerced.edu/mhyang/), " Deep Regression Tracking with Shrinkage Loss", ECCV 2018. 

![](../master/images/framework.png)

The pipeline is built upon the FCNT tracker and CRT paper (CRT: Convolutional Regression for Visual Tracking) 
https://github.com/scott89/FCNT
### Abstract
Regression trackers directly learn a mapping from regularly dense samples of target objects to soft labels, which are usually generated by a Gaussian function, to estimate target positions. Due to the potential for fast-tracking and easy implementation, regression trackers have received increasing attention recently. However, state-of-the-art deep regression trackers do not perform as well as discriminative correlation filters (DCFs) trackers. We identify the main bottleneck of training regression networks as extreme foreground-background data imbalance. To balance training data, we propose a novel shrinkage loss to penalize the importance of easy training data.  Additionally, we apply residual connections to fuse multiple convolutional layers as well as their output response maps. Without bells and whistles, the proposed deep regression tracking method performs favorably against state-of-the-art trackers, especially in comparison with DCFs trackers, on five benchmark datasets including OTB-2013, OTB-2015, Temple-128, UAV-123 and VOT-2016.


:tada:

Pytorch version is release:
--
Requirement
--
Python 2.7 (I use Anaconda 2.* here. If you use Python3, you may get the very different results!)
Python-opencv
PyTorch 0.40
other common packages such as numpy, etc


## Data Preparison
*Download ILSVRC15, and unzip it (let's assume that $ILSVRC2015_Root is the path to your ILSVRC2015)
  *Move $ILSVRC2015_Root/Data/VID/val into $ILSVRC2015_Root/Data/VID/train/, so we have five sub-folders in $ILSVRC2015_Root/Data/VID/train/
  *It is a good idea to change the names of five sub-folders in $ILSVRC2015_Root/Data/VID/train/ to a, b, c, d, and e
Move $ILSVRC2015_Root/Annotations/VID/val into $ILSVRC2015_Root/Annotations/VID/train/, so we have five sub-folders in $ILSVRC2015_Root/Annotations/VID/train/
  *Change the names of five sub-folders in $ILSVRC2015_Root/Annotations/VID/train/ to a, b, c, d and e, respectively
*Generate image crops
  *cd $SiamFC-PyTorch/ILSVRC15-curation/ (Assume you've downloaded the rep and its path is $SiamFC-PyTorch)
  *change vid_curated_path in gen_image_crops_VID.py to save your crops
  *run $python gen_image_crops_VID.py (I run it in PyCharm), then you can check the cropped images in your saving path (i.e., vid_curated_path)
*Generate imdb for training and validation
  *cd $SiamFC-PyTorch/ILSVRC15-curation/
  *change vid_root_path and vid_curated_path to your custom path in gen_imdb_VID.py
  *run $python gen_imdb_VID.py, then you will get two json files imdb_video_train.json (~ 430MB) and imdb_video_val.json (~ 28MB) in current folder, which are used for training and validation

### Results
The tracking results can be download [here](https://github.com/chaoma99/DSLT).
If you find the code useful, please cite
## citation
@inproceedings{lu2018deep,

  title={Deep Regression Tracking with Shrinkage Loss},
  
  author={Lu, Xiankai and Ma, Chao and Ni, Bingbing and Yang, Xiaokang and Reid, Ian and Yang, Ming-Hsuan},
  
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  
  pages={353--369},
  
  year={2018}
}


